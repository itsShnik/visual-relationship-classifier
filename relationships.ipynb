{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The structure of the code:\n",
    "\n",
    "- First we need to create the dataset from the precomputed bounding boxes for vqa dataset.\n",
    "- Then create a dataloader to load the bounding box pairs\n",
    "- The the Evaluation part\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports go here\n",
    "import numpy as np\n",
    "import json\n",
    "import base64\n",
    "import copy\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from vis_rel.function.config import config, update_config\n",
    "from vis_rel.modules.frcnn_classifier import Net\n",
    "\n",
    "update_config('cfgs/vis_rel/frcnn.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_rectangles(pil_img, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for (xmin, ymin, xmax, ymax), c in zip(boxes, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Dataset creation part\n",
    "\"\"\"\n",
    "\n",
    "sample_boxes = 20\n",
    "\n",
    "def createBoundingBoxPairs(path, verbose = False):\n",
    "    if verbose:\n",
    "        start_time = time.time()\n",
    "    image_id = int(path.split('/')[-1].replace('.json', ''))\n",
    "    objects = json.load(open(path))\n",
    "    im_info = (objects['image_w'], objects['image_h'])\n",
    "    \n",
    "    temp_bb_pairs = []\n",
    "    \n",
    "    boxes = np.frombuffer(base64.decodebytes(objects['boxes'].encode()), dtype=np.float32).reshape((objects['num_boxes'], -1))\n",
    "    \n",
    "    boxes = random.sample(list(boxes), min(len(boxes), sample_boxes))\n",
    "    \n",
    "    if verbose:\n",
    "        # plot objects on images\n",
    "        img = Image.open(glob.glob('data/coco/*/*' + str(image_id) + '.jpg')[0])\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        plot_rectangles(img, boxes)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        for j in range(len(boxes)):\n",
    "            if i != j:\n",
    "                temp_pair_dic = {\n",
    "                    'subj_bbox': boxes[i],\n",
    "                    'obj_bbox': boxes[j],\n",
    "                    'union_bbox': np.array([min(boxes[i][0], boxes[j][0]), min(boxes[i][1], boxes[j][1]), max(boxes[i][2], boxes[j][2]), max(boxes[i][3], boxes[j][3])], dtype=np.float32),\n",
    "                    'im_info': im_info,\n",
    "                    'image_id': image_id\n",
    "                }\n",
    "                temp_bb_pairs.append(temp_pair_dic)\n",
    "    if verbose:            \n",
    "        end_time = time.time()\n",
    "        print('Time taken in image id: {} is {}'.format(image_id, end_time-start_time))\n",
    "                \n",
    "    return temp_bb_pairs\n",
    "\n",
    "\n",
    "def createBoundingBoxPairDataset():\n",
    "    # both training and testing bb pairs\n",
    "    bb_pairs = []\n",
    "    \n",
    "    objs_path_list = glob.glob('data/coco/vgbua_res101_precomputed/*faster_rcnn_genome/*.json')[:128]\n",
    "    \n",
    "    # now for each image create the bb pairs\n",
    "    count = 0\n",
    "    total = len(objs_path_list)\n",
    "    for path in objs_path_list:\n",
    "        bb_pairs.extend(createBoundingBoxPairs(path, verbose = False))\n",
    "        count += 1\n",
    "        print(\"\\rImages done : {} / {}\".format(count, total), end=\"  \")\n",
    "        \n",
    "    return bb_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images done : 128 / 128  Length of bb pairs is : 47692\n"
     ]
    }
   ],
   "source": [
    "bb_pairs_dataset = createBoundingBoxPairDataset()\n",
    "print(\"Length of bb pairs is : {}\".format(len(bb_pairs_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to pickle file\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(\"saving to pickle file\")\n",
    "pickle.dump(bb_pairs_dataset, open('data/bb_pairs_dataset_tiny.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try on a single image\n",
    "img = Image.open('data/coco/train2014/COCO_train2014_000000291797.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some method to save the list of dictionaries containing numpy arrays\n",
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataloader for the dataset\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "class DatasetLoader(Data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, bb_pairs_dataset):\n",
    "        \n",
    "        self.image_dir = path\n",
    "        \n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize,\n",
    "        ])\n",
    "        \n",
    "        self.bb_pairs = bb_pairs_dataset[:1024]       \n",
    "        self.data_size = len(self.bb_pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bb_pair = self.bb_pairs[idx]\n",
    "        \n",
    "        img = Image.open(glob.glob(self.image_dir + '/*/*' + str(bb_pair['image_id']) + '.jpg')[0])\n",
    "        img = img.convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        inputs = {}\n",
    "        \n",
    "        # convert inputs to tensors\n",
    "        inputs['subj_bbox'] = torch.from_numpy(bb_pair['subj_bbox'])\n",
    "        inputs['obj_bbox'] = torch.from_numpy(bb_pair['obj_bbox'])\n",
    "        inputs['union_bbox'] = torch.from_numpy(bb_pair['union_bbox'])\n",
    "        \n",
    "        inputs['im_info'] = torch.tensor(bb_pair['im_info'])\n",
    "        inputs['image'] = img\n",
    "        \n",
    "        # image_id\n",
    "        image_ids = torch.tensor(bb_pair['image_id'])\n",
    "        \n",
    "        return inputs, image_ids\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The evaluator\n",
    "\"\"\"\n",
    "# batch_size\n",
    "batch_size = 16\n",
    "\n",
    "# Load the dataset\n",
    "val_dataset = DatasetLoader('data/coco', bb_pairs_dataset)\n",
    "\n",
    "val_dataloader = Data.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size,\n",
    "                    shuffle = False,\n",
    "                    num_workers = 4,\n",
    "                    pin_memory = True,\n",
    "                    sampler = None,\n",
    "                    drop_last = True\n",
    "                )\n",
    "\n",
    "print('Loaded the dataset')\n",
    "\n",
    "# os env\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'\n",
    "# Load the net\n",
    "model = Net(config)\n",
    "\n",
    "# define a softmax obj\n",
    "soft = nn.Softmax(-1)\n",
    "\n",
    "# Load the state dict\n",
    "path = 'output/output/vis_rel/ckpt_frcnn_train+val_low_lr_epoch7.pkl'\n",
    "state_dict = torch.load(path, map_location=torch.device('cpu'))['state_dict']\n",
    "new_state_dict = {k.replace('module.', ''):state_dict[k] for k in state_dict}\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.eval()\n",
    "print(\"here\")\n",
    "model.to(torch.device('cuda:4'))\n",
    "print('not here')\n",
    "model = nn.DataParallel(model, device_ids = ['cuda:4','cuda:5','cuda:6','cuda:7'])\n",
    "\n",
    "# Load the idx to label relationship\n",
    "rel_classes = json.load(open('data/relationship_classes.json'))\n",
    "class_rel = {v:k for k, v in rel_classes.items()}\n",
    "print(class_rel)\n",
    "\n",
    "# define a threshold\n",
    "threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello there\")\n",
    "import codecs\n",
    "relationships = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (\n",
    "            inputs,\n",
    "            image_ids\n",
    "        ) in enumerate(val_dataloader):\n",
    "\n",
    "        print(\"step is : {}\\n\".format(step))\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.cuda()\n",
    "\n",
    "        feats, pred = model(inputs)\n",
    "\n",
    "        # softmax over pred\n",
    "        pred = soft(pred)\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "            pred_ind = int(torch.argmax(pred[i]))\n",
    "            pred_val = torch.max(pred[i])\n",
    "\n",
    "            if pred_ind < 20 and pred_val > threshold:\n",
    "                print(\"The prediction index is: {}\".format(pred_ind))\n",
    "                print(\"the max prediction value is {}\".format(pred_val))\n",
    "                print(\"The relationship is {}\".format(str(class_rel[int(pred_ind)])))\n",
    "                temp_rel = {\n",
    "                    'predicate': str(class_rel[int(pred_ind)]),\n",
    "                    'features': feats.cpu().detach().tolist(),\n",
    "                    'subj_bbox': inputs['subj_bbox'][i].cpu().detach().tolist(),\n",
    "                    'obj_bbox': inputs['obj_bbox'][i].cpu().detach().tolist()\n",
    "                }\n",
    "\n",
    "                if str(int(image_ids[i])) not in relationships:\n",
    "                    relationships[str(int(image_ids[i]))] = []\n",
    "\n",
    "                relationships[str(int(image_ids[i]))].append(temp_rel)\n",
    "\n",
    "        print(\"\\rProgress {}/{}\".format(step, val_dataset.data_size/batch_size))\n",
    "\n",
    "\n",
    "for k, v in realtionships:\n",
    "    print(\"\\rImage id : {}\".format(str(k)), end=' ')\n",
    "    pickle.dump(v, open('data/coco/vqa_relationships/' + str(k) + '.pkl', 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(relationships.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
